{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data sets from multiple CCDs to make a more diverse data set.\n",
    "\n",
    "**Author:** Aram Lee\n",
    "**Date:** 2025-03-06\n",
    "**File Name:** Concatenator.ipynb\n",
    "\n",
    "### [Description]\n",
    "Using a data set from just one CCD was not enough to provide background/moving object variations to the model, and hence we used a concatenated data set from at least 3 CCDs (44 exposures per CCD). We also select pairs with desired labels and object magnitudes on this notebook.\n",
    "\n",
    "### [Required Libraries]\n",
    "- numpy: 1.26.4\n",
    "\n",
    "### [Workflow]  \n",
    "\n",
    "Steps 1-3 are for training the model, and steps 4-6 are for using the model to detect TNOs.\n",
    "\n",
    "|Step|File|Input|Output|Purpose|\n",
    "|-|-|-|-|-|\n",
    "|1|ImageCutter.ipynb|.fits (with artificial moving objects), .plantlist (artificial objects info)| .npy|Extract sub-images for training|\n",
    "|2|Concatenator.ipynb **(Here)**|.npy (sub-images from ImageCutter)|.npy|Prepare dataset for training|\n",
    "|3|Trainer.ipynb|.npy (dataset from Concatenator), .npy (target information)|.h5 (trained CNN models)|Train the model|\n",
    "|-|-|-|-|-|\n",
    "|4|ImageCutter.ipynb|.fits (without artificial moving objects)|.npy|Extract sub-images for detection|\n",
    "|5|Predictor.ipynb|.npy (sub-images from ImageCutter), .npy (target info), .h5 (model)|.npy|Apply trained model to detect objects|\n",
    "|6a|Link_sources_to_objects.py|.npy (classification and regression output from Predictor)|.npy|Detect moving objects (linear fitting method)|\n",
    "|6b|CandidateFinder.ipynb|.npy (classification output from Predictor), .npy (sub-images, target info)|.csv|Detect moving objects (scoring method)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import permutation as perm\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiple numpy arrays\n",
    "\n",
    "def load_data(prefixes, path='trainingsets/'):\n",
    "    return [np.load(f'{path}inp_{p}_P99NN.npy', allow_pickle=True) for p in prefixes], \\\n",
    "           [np.load(f'{path}tar_{p}_P99NN.npy', allow_pickle=True) for p in prefixes]\n",
    "\n",
    "img, tar = load_data(['ch05', 'ch10', 'ch20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts only the infos that will be used on this research.\n",
    "# target: (p1, p2, x1, y1, x2, y2, mag1, mag2)\n",
    "\n",
    "def tar_maker(var):\n",
    "    return [[i[0], i[1], i[2][0], i[2][1], i[3][0], i[3][1], i[6][7], i[7][7]] for i in var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = [tar_maker(t) for t in tar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a balanced set for binary labels [11, 10, 01, 00] with a ratio of 4:1:1:4. This part filters some of the data set.\n",
    "\n",
    "def ind(var):\n",
    "    where = {\"11\": [], \"10\": [], \"01\": [], \"00\": []}\n",
    "    \n",
    "    for i, e in enumerate(var):\n",
    "        if np.logical_and(e[:2] == [1, 1], 0 < e[6] <= 25):\n",
    "            where[\"11\"].append(i)\n",
    "        elif np.logical_and(e[:2] == [1, 0], 0 < e[6] <= 25):\n",
    "            where[\"10\"].append(i)\n",
    "        elif np.logical_and(e[:2] == [0, 1], 0 < e[7] <= 25):\n",
    "            where[\"01\"].append(i)\n",
    "        elif e[:2] == [0, 0]:\n",
    "            where[\"00\"].append(i)\n",
    "    \n",
    "    permuted = {k: perm(v) for k, v in where.items()}\n",
    "    min_count = min(len(permuted[\"11\"]), len(permuted[\"00\"]))\n",
    "    permuted[\"10\"] = permuted[\"10\"][:ceil(min_count / 4)]\n",
    "    permuted[\"01\"] = permuted[\"01\"][:ceil(min_count / 4)]\n",
    "    permuted[\"00\"] = permuted[\"00\"][:min_count]\n",
    "    permuted[\"11\"] = permuted[\"11\"][:min_count]\n",
    "    \n",
    "    return tuple(permuted.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ind function to categorize and randomize the indices.\n",
    "\n",
    "categories = [ind(t) for t in tar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set from each CCD and concatenate them into one train/validation set.\n",
    "\n",
    "# Split data into train and test\n",
    "def split(var, ratio=0.8):\n",
    "    cut = ceil(len(var) * ratio)\n",
    "    return var[:cut], var[cut:]\n",
    "\n",
    "# Concatenate train and test sets\n",
    "def concat(var):\n",
    "    train, test = zip(*(split(v) for v in var))\n",
    "    return np.concatenate(train), np.concatenate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the concat function to get the indicies of thetraining and validation set.\n",
    "Train, Test = zip(*(concat(cat) for cat in categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built indicies to concatenate the images and targets from 3 different CCDs, and save them.\n",
    "\n",
    "np.save('trainingsets/M_img_train.npy', np.concatenate([np.array(i)[t] for i, t in zip(img, Train)]))\n",
    "np.save('trainingsets/M_tar_train.npy', np.concatenate([np.array(i)[t] for i, t in zip(tar, Train)]))\n",
    "np.save('trainingsets/M_img_test.npy', np.concatenate([np.array(i)[t] for i, t in zip(tar, Test)]))\n",
    "np.save('trainingsets/M_tar_test.npy', np.concatenate([np.array(i)[t] for i, t in zip(tar, Test)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
